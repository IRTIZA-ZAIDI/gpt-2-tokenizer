{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "301e844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "# Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    "\n",
    "# * Why can't LLM spell words? Tokenization.\n",
    "# * Why can't LLM do super simple string processing tasks like reversing a string? Tokenization.\n",
    "# * Why is LLM worse at non-English languages (e.g. Japanese)? Tokenization.\n",
    "# * Why is LLM bad at simple arithmetic? Tokenization.\n",
    "# * Why did GPT-2 have more than necessary trouble coding in Python? Tokenization.\n",
    "# * Why did my LLM abruptly halt when it sees the string \"<|endoftext|>\"? Tokenization.\n",
    "# * What is this weird warning I get about a \"trailing whitespace\"? Tokenization.\n",
    "# * Why the LLM break if I ask it about \"SolidGoldMagikarp\"? Tokenization.\n",
    "# * Why should I prefer to use YAML over JSON with LLMs? Tokenization.\n",
    "# * Why is LLM not actually end-to-end language modeling? Tokenization.\n",
    "# * What is the real root of suffering? Tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ffe1afac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50504,\n",
       " 45397,\n",
       " 54616,\n",
       " 49464,\n",
       " 50836,\n",
       " 32,\n",
       " 128075,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in \"ÏïàÎÖïÌïòÏÑ∏Ïöî üëã (hello in Korean!)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8522ef5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50504"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"Ïïà\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f40efba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 149,\n",
       " 136,\n",
       " 235,\n",
       " 133,\n",
       " 149,\n",
       " 237,\n",
       " 149,\n",
       " 152,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 236,\n",
       " 154,\n",
       " 148,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 145,\n",
       " 139,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"ÏïàÎÖïÌïòÏÑ∏Ïöî üëã (hello in Korean!)\".encode('utf-8'))\n",
    "# only byte stream of 2^8 = 256 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176beab",
   "metadata": {},
   "source": [
    "# BPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a23493a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___\n",
      "Unicode! üòà UNICODE? üÜÑüÖΩüÖ∏üÖ≤üÖæüÖ≥üÖ¥! üòÅ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.\n",
      "length: 529\n",
      "___\n",
      "[85, 110, 105, 99, 111, 100, 101, 33, 32, 240, 159, 152, 136, 32, 85, 78, 73, 67, 79, 68, 69, 63, 32, 240, 159, 134, 132, 240, 159, 133, 189, 240, 159, 133, 184, 240, 159, 133, 178, 240, 159, 133, 190, 240, 159, 133, 179, 240, 159, 133, 180, 33, 32, 240, 159, 152, 129, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "length: 566\n"
     ]
    }
   ],
   "source": [
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "\n",
    "text = (\n",
    "    \"Unicode! üòà UNICODE? üÜÑüÖΩüÖ∏üÖ≤üÖæüÖ≥üÖ¥! üòÅ \"\n",
    "    \"The very name strikes fear and awe into the hearts of programmers worldwide. \"\n",
    "    \"We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äî\"\n",
    "    \"like using wchar_t for all the strings, right?). But Unicode can be abstruse, \"\n",
    "    \"and diving into the thousand-page Unicode Standard plus its dozens of supplementary \"\n",
    "    \"annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame \"\n",
    "    \"programmers for still finding the whole thing mysterious, even 30 years after \"\n",
    "    \"Unicode‚Äôs inception.\"\n",
    ")\n",
    "\n",
    "# Encode to UTF-8 bytes\n",
    "tokens = text.encode(\"utf-8\")  # raw bytes\n",
    "tokens = list(map(int, tokens))  # convert bytes to integers (0‚Äì255)\n",
    "\n",
    "print(\"___\")\n",
    "print(text)\n",
    "print(\"length:\", len(text))\n",
    "print(\"___\")\n",
    "print(tokens)\n",
    "print(\"length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0d0d82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20, (101, 32)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (240, 159)), (9, (32, 116)), (8, (116, 104)), (7, (97, 114)), (6, (159, 133)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (109, 101)), (6, (104, 101)), (6, (101, 114)), (6, (100, 101)), (6, (32, 105)), (5, (226, 128)), (5, (117, 115)), (5, (115, 116)), (5, (111, 100)), (5, (110, 105)), (5, (110, 32)), (5, (105, 99)), (5, (99, 111)), (5, (85, 110)), (5, (44, 32)), (5, (32, 115)), (5, (32, 85)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (110, 116)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (97, 109)), (4, (32, 119)), (4, (32, 111)), (4, (32, 102)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (114, 101)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 101)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (97, 116)), (3, (46, 32)), (3, (32, 240)), (3, (32, 112)), (3, (32, 109)), (3, (32, 100)), (3, (32, 98)), (2, (159, 152)), (2, (128, 153)), (2, (121, 32)), (2, (119, 104)), (2, (119, 101)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 109)), (2, (108, 105)), (2, (107, 101)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 101)), (2, (97, 108)), (2, (33, 32)), (2, (32, 114)), (2, (32, 110)), (2, (32, 99)), (1, (190, 240)), (1, (189, 240)), (1, (184, 240)), (1, (180, 33)), (1, (179, 240)), (1, (178, 240)), (1, (159, 134)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 136)), (1, (152, 129)), (1, (148, 108)), (1, (136, 32)), (1, (134, 132)), (1, (133, 190)), (1, (133, 189)), (1, (133, 184)), (1, (133, 180)), (1, (133, 179)), (1, (133, 178)), (1, (132, 240)), (1, (129, 32)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 100)), (1, (108, 97)), (1, (107, 110)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (103, 115)), (1, (103, 101)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (101, 33)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 101)), (1, (85, 78)), (1, (84, 104)), (1, (83, 116)), (1, (79, 68)), (1, (78, 73)), (1, (73, 67)), (1, (73, 32)), (1, (69, 63)), (1, (68, 69)), (1, (67, 79)), (1, (66, 117)), (1, (63, 41)), (1, (63, 32)), (1, (51, 48)), (1, (48, 32)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (32, 226)), (1, (32, 121)), (1, (32, 118)), (1, (32, 117)), (1, (32, 108)), (1, (32, 107)), (1, (32, 104)), (1, (32, 101)), (1, (32, 87)), (1, (32, 84)), (1, (32, 83)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51)), (1, (32, 40))]\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    # IMPORTANT\n",
    "    for pair in zip(ids, ids[1:]): # python way of iterating consecutive elements \n",
    "        counts[pair] = counts.get(pair, 0) + 1 \n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "# print(stats)\n",
    "print(sorted(((v, k) for k,v in stats.items()),reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c436ba4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101) , chr(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0552332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "053de137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if we are NOT at last position AND the pair matches replace it\n",
    "        if i < len(ids) -1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2 \n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "print(merge([5,6,6,7,9,1],(6,7),99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2d3f8877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85, 110, 105, 99, 111, 100, 101, 33, 32, 240, 159, 152, 136, 32, 85, 78, 73, 67, 79, 68, 69, 63, 32, 240, 159, 134, 132, 240, 159, 133, 189, 240, 159, 133, 184, 240, 159, 133, 178, 240, 159, 133, 190, 240, 159, 133, 179, 240, 159, 133, 180, 33, 32, 240, 159, 152, 129, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "length 546\n"
     ]
    }
   ],
   "source": [
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(tokens2)\n",
    "print('length', len(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c458a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making text longer and taking whole blog as text\n",
    "# https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"A Programmer‚Äôs Introduction to Unicode\\nMarch 3, 2017 ¬∑ Coding ¬∑ 25 Comments\\n\\nÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫‚Äåüá≥‚ÄåüáÆ‚Äåüá®‚Äåüá¥‚Äåüá©‚Äåüá™! üòÑ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.\\n\\nA few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I‚Äôll give an introduction to it from a programmer‚Äôs point of view.\\n\\nI‚Äôm going to focus on the character set and what‚Äôs involved in working with strings and files of Unicode text. However, in this article I‚Äôm not going to talk about fonts, text layout/shaping/rendering, or localization in detail‚Äîthose are separate issues, beyond my scope (and knowledge) here.\\n\\nDiversity and Inherent Complexity\\nThe Unicode Codespace\\nCodespace Allocation\\nScripts\\nUsage Frequency\\nEncodings\\nUTF-8\\nUTF-16\\nCombining Marks\\nCanonical Equivalence\\nNormalization Forms\\nGrapheme Clusters\\nAnd More‚Ä¶\\n\\nDiversity and Inherent Complexity\\nAs soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. It‚Äôs not just that Unicode contains a much larger number of characters, although that‚Äôs part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere ‚Äúcharacter set‚Äù to be.\\n\\nWhen confronting all this complexity, especially as an engineer, it‚Äôs hard not to find oneself asking, ‚ÄúWhy do we need all this? Is this really necessary? Couldn‚Äôt it be simplified?‚Äù\\n\\nHowever, Unicode aims to faithfully represent the entire world‚Äôs writing systems. The Unicode Consortium‚Äôs stated goal is ‚Äúenabling people around the world to use computers in any language‚Äù. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and there‚Äôs still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.\\n\\nGiven this enormous diversity, it‚Äôs inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems.\\n\\nThe Unicode Codespace\\nLet‚Äôs start with some general orientation. The basic elements of Unicode‚Äîits ‚Äúcharacters‚Äù, although that term isn‚Äôt quite right‚Äîare called code points. Code points are identified by number, customarily written in hexadecimal with the prefix ‚ÄúU+‚Äù, such as U+0041 ‚ÄúA‚Äù or U+03B8 ‚ÄúŒ∏‚Äù.\\n\\nCanonical Equivalence\\nIn Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express ‚Äúthe same‚Äù string‚Äîdifferent sequences of code points that result in the same user-perceived characters.\\n\\nNormalization Forms\\nTo address this problem, Unicode defines normalization forms such as NFC and NFD.\\n\\nGrapheme Clusters\\nUnicode formalizes the notion of a grapheme cluster: a sequence of one or more code points that form a single user-perceived character.\\n\\nUnicode is a fascinating and complex system, but it enables software to work correctly for billions of people across languages and scripts worldwide.\"\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to integer in range 0..255 for convinence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b188daa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (115, 32) into a new token 257\n",
      "merging (105, 110) into a new token 258\n",
      "merging (101, 114) into a new token 259\n",
      "merging (116, 32) into a new token 260\n",
      "merging (226, 128) into a new token 261\n",
      "merging (116, 104) into a new token 262\n",
      "merging (99, 111) into a new token 263\n",
      "merging (97, 114) into a new token 264\n",
      "merging (100, 32) into a new token 265\n",
      "merging (44, 32) into a new token 266\n",
      "merging (111, 114) into a new token 267\n",
      "merging (101, 110) into a new token 268\n",
      "merging (97, 110) into a new token 269\n",
      "merging (97, 108) into a new token 270\n",
      "merging (111, 110) into a new token 271\n",
      "merging (258, 103) into a new token 272\n",
      "merging (263, 100) into a new token 273\n",
      "merging (115, 116) into a new token 274\n",
      "merging (105, 116) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    # IMPORTANT\n",
    "    for pair in zip(ids, ids[1:]):  # python way of iterating consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if we are NOT at last position AND the pair matches replace it\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# -----\n",
    "vocab_size = 276 # desired final vocab size \n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) # copy so that we dont destroy original text \n",
    "\n",
    "merges = {} # (int, int) --> int\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i \n",
    "    print(f'merging {pair} into a new token {idx}')\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3b0ff3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 3797\n",
      "ids length: 2947\n",
      "compression ratio: 1.29X\n"
     ]
    }
   ],
   "source": [
    "print('tokens length:', len(tokens))\n",
    "print('ids length:', len(ids))\n",
    "print(f'compression ratio: {len(tokens)/len(ids):.2f}X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2176fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram = \"\"\"\n",
    "Raw text (Unicode code point sequence)\n",
    "              |\n",
    "              v\n",
    "         +-----------+\n",
    "         | Tokenizer |\n",
    "         +-----------+\n",
    "              |\n",
    "              v\n",
    "        Token sequence\n",
    "              |\n",
    "              v\n",
    "         +-----------+\n",
    "         |    LLM    |\n",
    "         +-----------+\n",
    "\n",
    "Note:\n",
    "- The tokenizer is a separate, independent module from the LLM.\n",
    "- It is trained (e.g., using BPE) on raw text.\n",
    "- It converts raw Unicode text ‚Üî token sequences.\n",
    "- The LLM only ever sees tokens, never raw text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99a82d",
   "metadata": {},
   "source": [
    "# decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "789a385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a seq of integers in range [0,vocab_size], whats the text?\n",
    "\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "# and now also going up the merge tree\n",
    "for (po, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[po] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    # given ids (list of integer), return python string\n",
    "    # b\"\"        # empty bytes object\n",
    "    # \"\"         # empty string\n",
    "    # Because vocab[idx] is a bytes object, not a string.\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors='replace')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c5c3febc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n"
     ]
    }
   ],
   "source": [
    "print(decode([67]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8b927f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÔøΩ\n"
     ]
    }
   ],
   "source": [
    "print(decode([128])) # cant because we dont conform to utf-8 formats (see wiki)\n",
    "# need to add errors='replace'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182b6b5",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5263f6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (115, 32): 257,\n",
       " (105, 110): 258,\n",
       " (101, 114): 259,\n",
       " (116, 32): 260,\n",
       " (226, 128): 261,\n",
       " (116, 104): 262,\n",
       " (99, 111): 263,\n",
       " (97, 114): 264,\n",
       " (100, 32): 265,\n",
       " (44, 32): 266,\n",
       " (111, 114): 267,\n",
       " (101, 110): 268,\n",
       " (97, 110): 269,\n",
       " (97, 108): 270,\n",
       " (111, 110): 271,\n",
       " (258, 103): 272,\n",
       " (263, 100): 273,\n",
       " (115, 116): 274,\n",
       " (105, 116): 275}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "49859cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 32, 119, 267, 108, 100, 33]\n"
     ]
    }
   ],
   "source": [
    "# given the string, what are the tokens?\n",
    "\n",
    "def encode(text):\n",
    "    # given a string, return list of integers(token)\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        # From all adjacent pairs currently present, pick the pair that was merged earliest during training.\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges: \n",
    "            break #nothing can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "print(encode('Hello world!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1e22d971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b9d064fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode('hello')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "926ae53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cb45e8",
   "metadata": {},
   "source": [
    "# forced splits using regex patterns (gpt-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a2a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', ' how', \"'ve\", ' are', '.', '    ', ' you', '!!!?', '   ']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "# GPT-2 style regex for forced token splits\n",
    "gpt2pat = re.compile(\n",
    "    r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    ")\n",
    "#  ?\\p{L} -> space followed by letters from any languages\n",
    "# we split these elements and then find merges independent to these elements\n",
    "# enforcing some merges to not happen\n",
    "\n",
    "# ?\\p{N} -> letters and numbers are seperated\n",
    "# ve and re are seperated (as they are common)\n",
    "\n",
    "# ?[^\\s\\p{L}\\p{N}] --> followed by not letters or numbers (puntuations)\n",
    "\n",
    "# \\s+(?!\\S) -> extra white space are own elements\n",
    "\n",
    "# \\s -> just ending white space\n",
    "\n",
    "# spaces are never merged \n",
    "print(re.findall(gpt2pat, \"Hello world how've are.     you!!!?   \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662d6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 23748, 995, 10185]\n",
      "[220, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2 (DOES NOT MERGES SPACES)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"  hello world!!!\"))\n",
    "\n",
    "# GPT-4 (MERGES SPACES)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"  hello world!!!\"))\n",
    "\n",
    "\n",
    "# see regexes in https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da72ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4 style regex for forced token splits\n",
    "gpt4pat = re.compile(\n",
    "    r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}++|\\p{N}{1,3}+| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*+|\\s++$|\\s*[\\r\\n]|\\s+(?!\\S)|\\s\"\"\"\n",
    ")\n",
    "\n",
    "# \\p{N}{1,3} -> only 1-3 numbers are merged\n",
    "# i:[sdmt]|ll|ve|re) -> 's , 'd etc are matched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ded57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/openai/gpt-2/blob/master/src/encoder.py \n",
    "# encoder and decoder of gpt2 \n",
    "# very similar to ours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "44f28348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to download these two files:\n",
    "# !wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
    "# !wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# load encoder (token -> id mapping)\n",
    "# equivalent to our vocab\n",
    "with open(\"encoder.json\", \"r\") as f:\n",
    "    encoder = json.load(f)\n",
    "\n",
    "# load BPE merge rules\n",
    "# equivalent to our merges\n",
    "with open(\"vocab.bpe\", \"r\", encoding=\"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "\n",
    "# each line after the first is a merge rule: \"token1 token2\"\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f67443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder) # 256 raw bytes token, 50,000 merges. 1 special token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fcfd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special token \n",
    "encoder['<|endoftext|>']\n",
    "# these are added outside of bpe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "80714fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(encoder)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf797d6",
   "metadata": {},
   "source": [
    "## adding new special tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712cb398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "\n",
    "# Get the base encoding\n",
    "# cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Create a new Encoding object by extending it\n",
    "# In production, load the arguments directly instead of accessing private attributes\n",
    "# See openai_public.py for examples of arguments for specific encodings\n",
    "\n",
    "# enc = tiktoken.Encoding(\n",
    "#     # If you're changing the set of special tokens, make sure to use a different name\n",
    "#     # It should be clear from the name what behaviour to expect.\n",
    "#     name=\"cl100k_im\",\n",
    "#     pat_str=cl100k_base._pat_str,\n",
    "#     mergeable_ranks=cl100k_base._mergeable_ranks,\n",
    "#     special_tokens={\n",
    "#         **cl100k_base._special_tokens,\n",
    "#         \"<|im_start|>\": 100264,\n",
    "#         \"<|im_end|>\": 100265,\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75bb20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different speical tokens for diff tokenizers \n",
    "# see openai_public.py of tiktokenizer\n",
    "# adding special token u need to update embedding layer and final projection layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39354a20",
   "metadata": {},
   "source": [
    "# sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2141ea46",
   "metadata": {},
   "source": [
    "## sentencepiece\n",
    "\n",
    "SentencePiece is commonly used because (unlike `tiktoken`) it can efficiently **both train and run inference** for BPE tokenizers. It is used in **LLaMA** and **Mistral** model families.\n",
    "\n",
    "GitHub: https://github.com/google/sentencepiece\n",
    "\n",
    "---\n",
    "\n",
    "### The big difference\n",
    "\n",
    "**SentencePiece runs BPE directly on Unicode code points**, not on UTF-8 bytes.\n",
    "\n",
    "It provides two important options:\n",
    "\n",
    "- **`character_coverage`**  \n",
    "  Controls how much of the Unicode space is explicitly modeled.  \n",
    "  Very rare characters (below this coverage threshold) are treated specially.\n",
    "\n",
    "- **`byte_fallback`**  \n",
    "  If enabled, rare Unicode code points are:\n",
    "  1. UTF-8 encoded\n",
    "  2. Then tokenized as raw bytes  \n",
    "  instead of being mapped to an `<unk>` token.\n",
    "\n",
    "---\n",
    "\n",
    "### TL;DR\n",
    "\n",
    "- **tiktoken**\n",
    "  - Converts text ‚Üí UTF-8 bytes\n",
    "  - Runs BPE on bytes\n",
    "\n",
    "- **sentencepiece**\n",
    "  - Runs BPE on Unicode code points\n",
    "  - Optionally falls back to UTF-8 bytes for rare characters\n",
    "  - Rarity is controlled by `character_coverage`\n",
    "\n",
    "*(Personal note: the byte-level approach used by tiktoken is often considered cleaner and more predictable.)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c44254c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# write a toy.txt file with some random text\n",
    "with open(\"toy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\n",
    "        \"SentencePiece is an unsupervised text tokenizer and detokenizer \"\n",
    "        \"mainly for Neural Network-based text generation systems.\\n\"\n",
    "        \"It supports subword units such as BPE and unigram language models.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cc36cc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: toy.txt\n",
      "  input_format: text\n",
      "  model_prefix: tok400\n",
      "  model_type: BPE\n",
      "  vocab_size: 400\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.99995\n",
      "  input_sentence_size: 200000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 8\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 1\n",
      "  required_chars: \n",
      "  byte_fallback: 1\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: identity\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: toy.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 2 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x00>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x01>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x02>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x03>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x04>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x05>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x06>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x07>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x08>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x09>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x10>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x11>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x12>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x13>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x14>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x15>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x16>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x17>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x18>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x19>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x20>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x21>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x22>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x23>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x24>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x25>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x26>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x27>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x28>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x29>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x30>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x31>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x32>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x33>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x34>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x35>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x36>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x37>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x38>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x39>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x40>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x41>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x42>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x43>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x44>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x45>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x46>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x47>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x48>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x49>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x50>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x51>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x52>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x53>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x54>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x55>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x56>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x57>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x58>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x59>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x60>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x61>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x62>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x63>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x64>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x65>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x66>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x67>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x68>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x69>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x70>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x71>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x72>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x73>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x74>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x75>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x76>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x77>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x78>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x79>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x80>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x81>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x82>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x83>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x84>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x85>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x86>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x87>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x88>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x89>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x90>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x91>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x92>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x93>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x94>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x95>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x96>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x97>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x98>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x99>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xED>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFF>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=188\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=33\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 2 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 2\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 24\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20 all=181 active=148 piece=ts\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=40 all=195 active=162 piece=gu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=60 all=204 active=171 piece=ode\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=80 all=207 active=174 piece=Piece\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=100 all=200 active=167 piece=‚ñÅsystems\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: tok400.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: tok400.vocab\n"
     ]
    }
   ],
   "source": [
    "# train a sentencepiece model on it\n",
    "# the settings here are (best effort) those used for training LLaMA 2\n",
    "\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "options = dict(\n",
    "    # input spec\n",
    "    input=\"toy.txt\",\n",
    "    input_format=\"text\",\n",
    "    # output spec\n",
    "    model_prefix=\"tok400\",  # output filename prefix\n",
    "    # algorithm spec\n",
    "    # BPE alg\n",
    "    model_type=\"bpe\",\n",
    "    vocab_size=400,\n",
    "    # normalization\n",
    "    normalization_rule_name=\"identity\",  # ew, turn off normalization\n",
    "    remove_extra_whitespaces=False,\n",
    "    input_sentence_size=200000000,  # max number of training sentences\n",
    "    max_sentence_length=4192,  # max number of bytes per sentence\n",
    "    seed_sentencepiece_size=1000000,\n",
    "    shuffle_input_sentence=True,\n",
    "    # rare word treatment\n",
    "    character_coverage=0.99995,\n",
    "    byte_fallback=True,\n",
    "    # merge rules\n",
    "    split_digits=True,\n",
    "    split_by_unicode_script=True,\n",
    "    split_by_whitespace=True,\n",
    "    split_by_number=True,\n",
    "    max_sentencepiece_length=16,\n",
    "    add_dummy_prefix=True,\n",
    "    allow_whitespace_only_pieces=True,\n",
    "    # special tokens\n",
    "    unk_id=0,  # the UNK token MUST exist\n",
    "    bos_id=1,  # the others are optional, set to -1 to turn off\n",
    "    eos_id=2,\n",
    "    pad_id=-1,\n",
    "    # systems\n",
    "    num_threads=os.cpu_count(),  # use ~all system resources\n",
    ")\n",
    "\n",
    "# train the model\n",
    "spm.SentencePieceTrainer.train(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58257179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', 0],\n",
       " ['<s>', 1],\n",
       " ['</s>', 2],\n",
       " ['<0x00>', 3],\n",
       " ['<0x01>', 4],\n",
       " ['<0x02>', 5],\n",
       " ['<0x03>', 6],\n",
       " ['<0x04>', 7],\n",
       " ['<0x05>', 8],\n",
       " ['<0x06>', 9],\n",
       " ['<0x07>', 10],\n",
       " ['<0x08>', 11],\n",
       " ['<0x09>', 12],\n",
       " ['<0x0A>', 13],\n",
       " ['<0x0B>', 14],\n",
       " ['<0x0C>', 15],\n",
       " ['<0x0D>', 16],\n",
       " ['<0x0E>', 17],\n",
       " ['<0x0F>', 18],\n",
       " ['<0x10>', 19],\n",
       " ['<0x11>', 20],\n",
       " ['<0x12>', 21],\n",
       " ['<0x13>', 22],\n",
       " ['<0x14>', 23],\n",
       " ['<0x15>', 24],\n",
       " ['<0x16>', 25],\n",
       " ['<0x17>', 26],\n",
       " ['<0x18>', 27],\n",
       " ['<0x19>', 28],\n",
       " ['<0x1A>', 29],\n",
       " ['<0x1B>', 30],\n",
       " ['<0x1C>', 31],\n",
       " ['<0x1D>', 32],\n",
       " ['<0x1E>', 33],\n",
       " ['<0x1F>', 34],\n",
       " ['<0x20>', 35],\n",
       " ['<0x21>', 36],\n",
       " ['<0x22>', 37],\n",
       " ['<0x23>', 38],\n",
       " ['<0x24>', 39],\n",
       " ['<0x25>', 40],\n",
       " ['<0x26>', 41],\n",
       " ['<0x27>', 42],\n",
       " ['<0x28>', 43],\n",
       " ['<0x29>', 44],\n",
       " ['<0x2A>', 45],\n",
       " ['<0x2B>', 46],\n",
       " ['<0x2C>', 47],\n",
       " ['<0x2D>', 48],\n",
       " ['<0x2E>', 49],\n",
       " ['<0x2F>', 50],\n",
       " ['<0x30>', 51],\n",
       " ['<0x31>', 52],\n",
       " ['<0x32>', 53],\n",
       " ['<0x33>', 54],\n",
       " ['<0x34>', 55],\n",
       " ['<0x35>', 56],\n",
       " ['<0x36>', 57],\n",
       " ['<0x37>', 58],\n",
       " ['<0x38>', 59],\n",
       " ['<0x39>', 60],\n",
       " ['<0x3A>', 61],\n",
       " ['<0x3B>', 62],\n",
       " ['<0x3C>', 63],\n",
       " ['<0x3D>', 64],\n",
       " ['<0x3E>', 65],\n",
       " ['<0x3F>', 66],\n",
       " ['<0x40>', 67],\n",
       " ['<0x41>', 68],\n",
       " ['<0x42>', 69],\n",
       " ['<0x43>', 70],\n",
       " ['<0x44>', 71],\n",
       " ['<0x45>', 72],\n",
       " ['<0x46>', 73],\n",
       " ['<0x47>', 74],\n",
       " ['<0x48>', 75],\n",
       " ['<0x49>', 76],\n",
       " ['<0x4A>', 77],\n",
       " ['<0x4B>', 78],\n",
       " ['<0x4C>', 79],\n",
       " ['<0x4D>', 80],\n",
       " ['<0x4E>', 81],\n",
       " ['<0x4F>', 82],\n",
       " ['<0x50>', 83],\n",
       " ['<0x51>', 84],\n",
       " ['<0x52>', 85],\n",
       " ['<0x53>', 86],\n",
       " ['<0x54>', 87],\n",
       " ['<0x55>', 88],\n",
       " ['<0x56>', 89],\n",
       " ['<0x57>', 90],\n",
       " ['<0x58>', 91],\n",
       " ['<0x59>', 92],\n",
       " ['<0x5A>', 93],\n",
       " ['<0x5B>', 94],\n",
       " ['<0x5C>', 95],\n",
       " ['<0x5D>', 96],\n",
       " ['<0x5E>', 97],\n",
       " ['<0x5F>', 98],\n",
       " ['<0x60>', 99],\n",
       " ['<0x61>', 100],\n",
       " ['<0x62>', 101],\n",
       " ['<0x63>', 102],\n",
       " ['<0x64>', 103],\n",
       " ['<0x65>', 104],\n",
       " ['<0x66>', 105],\n",
       " ['<0x67>', 106],\n",
       " ['<0x68>', 107],\n",
       " ['<0x69>', 108],\n",
       " ['<0x6A>', 109],\n",
       " ['<0x6B>', 110],\n",
       " ['<0x6C>', 111],\n",
       " ['<0x6D>', 112],\n",
       " ['<0x6E>', 113],\n",
       " ['<0x6F>', 114],\n",
       " ['<0x70>', 115],\n",
       " ['<0x71>', 116],\n",
       " ['<0x72>', 117],\n",
       " ['<0x73>', 118],\n",
       " ['<0x74>', 119],\n",
       " ['<0x75>', 120],\n",
       " ['<0x76>', 121],\n",
       " ['<0x77>', 122],\n",
       " ['<0x78>', 123],\n",
       " ['<0x79>', 124],\n",
       " ['<0x7A>', 125],\n",
       " ['<0x7B>', 126],\n",
       " ['<0x7C>', 127],\n",
       " ['<0x7D>', 128],\n",
       " ['<0x7E>', 129],\n",
       " ['<0x7F>', 130],\n",
       " ['<0x80>', 131],\n",
       " ['<0x81>', 132],\n",
       " ['<0x82>', 133],\n",
       " ['<0x83>', 134],\n",
       " ['<0x84>', 135],\n",
       " ['<0x85>', 136],\n",
       " ['<0x86>', 137],\n",
       " ['<0x87>', 138],\n",
       " ['<0x88>', 139],\n",
       " ['<0x89>', 140],\n",
       " ['<0x8A>', 141],\n",
       " ['<0x8B>', 142],\n",
       " ['<0x8C>', 143],\n",
       " ['<0x8D>', 144],\n",
       " ['<0x8E>', 145],\n",
       " ['<0x8F>', 146],\n",
       " ['<0x90>', 147],\n",
       " ['<0x91>', 148],\n",
       " ['<0x92>', 149],\n",
       " ['<0x93>', 150],\n",
       " ['<0x94>', 151],\n",
       " ['<0x95>', 152],\n",
       " ['<0x96>', 153],\n",
       " ['<0x97>', 154],\n",
       " ['<0x98>', 155],\n",
       " ['<0x99>', 156],\n",
       " ['<0x9A>', 157],\n",
       " ['<0x9B>', 158],\n",
       " ['<0x9C>', 159],\n",
       " ['<0x9D>', 160],\n",
       " ['<0x9E>', 161],\n",
       " ['<0x9F>', 162],\n",
       " ['<0xA0>', 163],\n",
       " ['<0xA1>', 164],\n",
       " ['<0xA2>', 165],\n",
       " ['<0xA3>', 166],\n",
       " ['<0xA4>', 167],\n",
       " ['<0xA5>', 168],\n",
       " ['<0xA6>', 169],\n",
       " ['<0xA7>', 170],\n",
       " ['<0xA8>', 171],\n",
       " ['<0xA9>', 172],\n",
       " ['<0xAA>', 173],\n",
       " ['<0xAB>', 174],\n",
       " ['<0xAC>', 175],\n",
       " ['<0xAD>', 176],\n",
       " ['<0xAE>', 177],\n",
       " ['<0xAF>', 178],\n",
       " ['<0xB0>', 179],\n",
       " ['<0xB1>', 180],\n",
       " ['<0xB2>', 181],\n",
       " ['<0xB3>', 182],\n",
       " ['<0xB4>', 183],\n",
       " ['<0xB5>', 184],\n",
       " ['<0xB6>', 185],\n",
       " ['<0xB7>', 186],\n",
       " ['<0xB8>', 187],\n",
       " ['<0xB9>', 188],\n",
       " ['<0xBA>', 189],\n",
       " ['<0xBB>', 190],\n",
       " ['<0xBC>', 191],\n",
       " ['<0xBD>', 192],\n",
       " ['<0xBE>', 193],\n",
       " ['<0xBF>', 194],\n",
       " ['<0xC0>', 195],\n",
       " ['<0xC1>', 196],\n",
       " ['<0xC2>', 197],\n",
       " ['<0xC3>', 198],\n",
       " ['<0xC4>', 199],\n",
       " ['<0xC5>', 200],\n",
       " ['<0xC6>', 201],\n",
       " ['<0xC7>', 202],\n",
       " ['<0xC8>', 203],\n",
       " ['<0xC9>', 204],\n",
       " ['<0xCA>', 205],\n",
       " ['<0xCB>', 206],\n",
       " ['<0xCC>', 207],\n",
       " ['<0xCD>', 208],\n",
       " ['<0xCE>', 209],\n",
       " ['<0xCF>', 210],\n",
       " ['<0xD0>', 211],\n",
       " ['<0xD1>', 212],\n",
       " ['<0xD2>', 213],\n",
       " ['<0xD3>', 214],\n",
       " ['<0xD4>', 215],\n",
       " ['<0xD5>', 216],\n",
       " ['<0xD6>', 217],\n",
       " ['<0xD7>', 218],\n",
       " ['<0xD8>', 219],\n",
       " ['<0xD9>', 220],\n",
       " ['<0xDA>', 221],\n",
       " ['<0xDB>', 222],\n",
       " ['<0xDC>', 223],\n",
       " ['<0xDD>', 224],\n",
       " ['<0xDE>', 225],\n",
       " ['<0xDF>', 226],\n",
       " ['<0xE0>', 227],\n",
       " ['<0xE1>', 228],\n",
       " ['<0xE2>', 229],\n",
       " ['<0xE3>', 230],\n",
       " ['<0xE4>', 231],\n",
       " ['<0xE5>', 232],\n",
       " ['<0xE6>', 233],\n",
       " ['<0xE7>', 234],\n",
       " ['<0xE8>', 235],\n",
       " ['<0xE9>', 236],\n",
       " ['<0xEA>', 237],\n",
       " ['<0xEB>', 238],\n",
       " ['<0xEC>', 239],\n",
       " ['<0xED>', 240],\n",
       " ['<0xEE>', 241],\n",
       " ['<0xEF>', 242],\n",
       " ['<0xF0>', 243],\n",
       " ['<0xF1>', 244],\n",
       " ['<0xF2>', 245],\n",
       " ['<0xF3>', 246],\n",
       " ['<0xF4>', 247],\n",
       " ['<0xF5>', 248],\n",
       " ['<0xF6>', 249],\n",
       " ['<0xF7>', 250],\n",
       " ['<0xF8>', 251],\n",
       " ['<0xF9>', 252],\n",
       " ['<0xFA>', 253],\n",
       " ['<0xFB>', 254],\n",
       " ['<0xFC>', 255],\n",
       " ['<0xFD>', 256],\n",
       " ['<0xFE>', 257],\n",
       " ['<0xFF>', 258],\n",
       " ['en', 259],\n",
       " ['an', 260],\n",
       " ['er', 261],\n",
       " ['or', 262],\n",
       " ['su', 263],\n",
       " ['te', 264],\n",
       " ['un', 265],\n",
       " ['‚ñÅan', 266],\n",
       " ['‚ñÅsu', 267],\n",
       " ['‚ñÅun', 268],\n",
       " ['Ne', 269],\n",
       " ['as', 270],\n",
       " ['ce', 271],\n",
       " ['de', 272],\n",
       " ['ed', 273],\n",
       " ['is', 274],\n",
       " ['iz', 275],\n",
       " ['ok', 276],\n",
       " ['ra', 277],\n",
       " ['ts', 278],\n",
       " ['xt', 279],\n",
       " ['‚ñÅm', 280],\n",
       " ['tok', 281],\n",
       " ['wor', 282],\n",
       " ['‚ñÅNe', 283],\n",
       " ['‚ñÅte', 284],\n",
       " ['eniz', 285],\n",
       " ['‚ñÅand', 286],\n",
       " ['‚ñÅuni', 287],\n",
       " ['‚ñÅtext', 288],\n",
       " ['enizer', 289],\n",
       " ['tokenizer', 290],\n",
       " ['BP', 291],\n",
       " ['It', 292],\n",
       " ['Pi', 293],\n",
       " ['ag', 294],\n",
       " ['ai', 295],\n",
       " ['at', 296],\n",
       " ['ch', 297],\n",
       " ['gu', 298],\n",
       " ['io', 299],\n",
       " ['ls', 300],\n",
       " ['ly', 301],\n",
       " ['ms', 302],\n",
       " ['pp', 303],\n",
       " ['sy', 304],\n",
       " ['‚ñÅS', 305],\n",
       " ['‚ñÅf', 306],\n",
       " ['‚ñÅg', 307],\n",
       " ['‚ñÅl', 308],\n",
       " ['BPE', 309],\n",
       " ['Pie', 310],\n",
       " ['age', 311],\n",
       " ['ain', 312],\n",
       " ['bas', 313],\n",
       " ['ent', 314],\n",
       " ['erv', 315],\n",
       " ['gra', 316],\n",
       " ['ion', 317],\n",
       " ['ode', 318],\n",
       " ['ral', 319],\n",
       " ['ste', 320],\n",
       " ['sup', 321],\n",
       " ['‚ñÅIt', 322],\n",
       " ['‚ñÅas', 323],\n",
       " ['‚ñÅde', 324],\n",
       " ['‚ñÅis', 325],\n",
       " ['‚ñÅsy', 326],\n",
       " ['angu', 327],\n",
       " ['bwor', 328],\n",
       " ['ence', 329],\n",
       " ['ener', 330],\n",
       " ['gram', 331],\n",
       " ['ised', 332],\n",
       " ['orts', 333],\n",
       " ['twor', 334],\n",
       " ['ural', 335],\n",
       " ['‚ñÅBPE', 336],\n",
       " ['‚ñÅfor', 337],\n",
       " ['Piece', 338],\n",
       " ['ainly', 339],\n",
       " ['ation', 340],\n",
       " ['based', 341],\n",
       " ['bword', 342],\n",
       " ['odels', 343],\n",
       " ['stems', 344],\n",
       " ['twork', 345],\n",
       " ['‚ñÅSent', 346],\n",
       " ['‚ñÅsuch', 347],\n",
       " ['‚ñÅsupp', 348],\n",
       " ['superv', 349],\n",
       " ['‚ñÅgener', 350],\n",
       " ['‚ñÅlangu', 351],\n",
       " ['‚ñÅunits', 352],\n",
       " ['‚ñÅNeural', 353],\n",
       " ['‚ñÅmainly', 354],\n",
       " ['‚ñÅmodels', 355],\n",
       " ['‚ñÅNetwork', 356],\n",
       " ['‚ñÅsubword', 357],\n",
       " ['‚ñÅsystems', 358],\n",
       " ['‚ñÅunigram', 359],\n",
       " ['encePiece', 360],\n",
       " ['‚ñÅlanguage', 361],\n",
       " ['‚ñÅsupports', 362],\n",
       " ['‚ñÅunsuperv', 363],\n",
       " ['‚ñÅtokenizer', 364],\n",
       " ['‚ñÅgeneration', 365],\n",
       " ['‚ñÅdetokenizer', 366],\n",
       " ['‚ñÅ', 367],\n",
       " ['e', 368],\n",
       " ['n', 369],\n",
       " ['s', 370],\n",
       " ['t', 371],\n",
       " ['a', 372],\n",
       " ['r', 373],\n",
       " ['i', 374],\n",
       " ['u', 375],\n",
       " ['o', 376],\n",
       " ['d', 377],\n",
       " ['g', 378],\n",
       " ['l', 379],\n",
       " ['m', 380],\n",
       " ['c', 381],\n",
       " ['k', 382],\n",
       " ['p', 383],\n",
       " ['.', 384],\n",
       " ['N', 385],\n",
       " ['P', 386],\n",
       " ['b', 387],\n",
       " ['w', 388],\n",
       " ['x', 389],\n",
       " ['y', 390],\n",
       " ['z', 391],\n",
       " ['-', 392],\n",
       " ['B', 393],\n",
       " ['E', 394],\n",
       " ['I', 395],\n",
       " ['S', 396],\n",
       " ['f', 397],\n",
       " ['h', 398],\n",
       " ['v', 399]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tok400.model')\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab\n",
    "# starts with special tokens, byte tokens, merges and then indiviual tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9e520fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[367, 398, 368, 379, 379, 376, 367, 239, 152, 139, 238, 136, 152, 240, 152, 155, 239, 135, 187, 239, 157, 151]\n"
     ]
    }
   ],
   "source": [
    "ids = sp.encode(\"hello ÏïàÎÖïÌïòÏÑ∏Ïöî\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef42a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅ', 'h', 'e', 'l', 'l', 'o', '‚ñÅ', '<0xEC>', '<0x95>', '<0x88>', '<0xEB>', '<0x85>', '<0x95>', '<0xED>', '<0x95>', '<0x98>', '<0xEC>', '<0x84>', '<0xB8>', '<0xEC>', '<0x9A>', '<0x94>']\n"
     ]
    }
   ],
   "source": [
    "print([sp.id_to_piece(idx) for idx in ids])\n",
    "# encoding didnt see ÏïàÎÖïÌïòÏÑ∏Ïöî these tokens during train\n",
    "# hence these are UNK(unknown) tokens\n",
    "# but since byte fallback is true, sentence piece fall backs to bytes\n",
    "# encodes them by utf-8 and then uses bytes token to represent these\n",
    "\n",
    "# if there is byte fall back\n",
    "# there will be no byte tokens and we will have more merges as more space to vocab_size: 400\n",
    "# but the final output of encoding will be ['‚ñÅ', 'h', 'e', 'l', 'l', 'o', '‚ñÅ','<unk> ]\n",
    "# [367, 398, 368, 379, 379, 376, 367, 0 ] -> <unk> is 0\n",
    "# all of ÏïàÎÖïÌïòÏÑ∏Ïöî is encoded as <unk>\n",
    "\n",
    "# sentence piece also converts spaces to _\n",
    "# we have a start at ['‚ñÅ', 'h', 'e', 'l', 'l' ... \n",
    "# becuase it uses a dummy prefix \n",
    "# converts: \n",
    "# world  -> this world by adding a prefix so that its similar to below [space]world\n",
    "# hello world\n",
    "# hence we get a _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b11ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This proto fully defines LLaMA-2 tokenization.\n",
    "# If any of these values differ, your tokenizer is not LLaMA-2 compatible.\n",
    "\n",
    "# normalizer_spec {\n",
    "#   name: \"identity\"\n",
    "#   precompiled_charsmap: \"\"\n",
    "#   add_dummy_prefix: true\n",
    "#   remove_extra_whitespaces: false\n",
    "#   normalization_rule_tsv: \"\"\n",
    "# }\n",
    "\n",
    "# trainer_spec {\n",
    "#   input: \"/large_experiments/theorem/datasets/MERGED/all.test1.merged\"\n",
    "#   model_prefix: \"spm_model_32k_200M_charcov099995_allowWS0_v2\"\n",
    "#   model_type: BPE\n",
    "#   vocab_size: 32000\n",
    "#   self_test_sample_size: 0\n",
    "#   input_format: \"text\"\n",
    "\n",
    "#   character_coverage: 0.99995\n",
    "#   input_sentence_size: 200000000\n",
    "#   seed_sentencepiece_size: 1000000\n",
    "#   shrinking_factor: 0.75\n",
    "\n",
    "#   num_threads: 80\n",
    "#   num_sub_iterations: 2\n",
    "\n",
    "#   max_sentence_length: 4192\n",
    "#   shuffle_input_sentence: true\n",
    "\n",
    "#   max_sentencepiece_length: 16\n",
    "#   split_by_unicode_script: true\n",
    "#   split_by_whitespace: true\n",
    "#   split_by_number: true\n",
    "#   treat_whitespace_as_suffix: false\n",
    "#   split_digits: true\n",
    "#   allow_whitespace_only_pieces: true\n",
    "\n",
    "#   vocabulary_output_piece_score: true\n",
    "#   hard_vocab_limit: true\n",
    "#   use_all_vocab: false\n",
    "\n",
    "#   byte_fallback: true\n",
    "#   required_chars: \"\"\n",
    "\n",
    "#   unk_id: 0\n",
    "#   bos_id: 1\n",
    "#   eos_id: 2\n",
    "#   pad_id: -1\n",
    "\n",
    "#   unk_surface: \" \\342\\201\\207 \"\n",
    "#   unk_piece: \"<unk>\"\n",
    "#   bos_piece: \"<s>\"\n",
    "#   eos_piece: \"</s>\"\n",
    "#   pad_piece: \"<pad>\"\n",
    "\n",
    "#   train_extremely_large_corpus: false\n",
    "\n",
    "#   enable_differential_privacy: false\n",
    "#   differential_privacy_noise_level: 0.0\n",
    "#   differential_privacy_clipping_threshold: 0\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# GPT-2 Tokenizer vs SentencePiece (ESSENTIAL DIFF)\n",
    "# ===============================================\n",
    "\n",
    "# -------------------------\n",
    "# 1. Core unit of operation\n",
    "# -------------------------\n",
    "# GPT-2 tokenizer:\n",
    "# - Operates on UTF-8 BYTES\n",
    "# - Text ‚Üí UTF-8 bytes ‚Üí BPE merges on bytes\n",
    "# - Every byte (0‚Äì255) is representable\n",
    "#\n",
    "# SentencePiece:\n",
    "# - Operates on UNICODE CODE POINTS\n",
    "# - Text ‚Üí Unicode chars ‚Üí BPE / Unigram on characters\n",
    "# - Optionally falls back to bytes for rare characters\n",
    "\n",
    "# -------------------------\n",
    "# 2. Unknown token behavior\n",
    "# -------------------------\n",
    "# GPT-2 tokenizer:\n",
    "# - NO <UNK> token ever\n",
    "# - All text is representable via bytes\n",
    "#\n",
    "# SentencePiece:\n",
    "# - Has <unk> token by default\n",
    "# - Can avoid <unk> only if byte_fallback=True\n",
    "\n",
    "# -------------------------\n",
    "# 3. Normalization\n",
    "# -------------------------\n",
    "# GPT-2 tokenizer:\n",
    "# - No normalization (raw bytes)\n",
    "# - Exact byte preservation\n",
    "#\n",
    "# SentencePiece:\n",
    "# - Normalization ON by default (NFKC etc.)\n",
    "# - LLaMA explicitly disables it (identity normalization)\n",
    "\n",
    "# -------------------------\n",
    "# 4. Training vs inference\n",
    "# -------------------------\n",
    "# GPT-2 tokenizer:\n",
    "# - Training handled externally (OpenAI tools)\n",
    "# - tiktoken is INFERENCE-ONLY\n",
    "#\n",
    "# SentencePiece:\n",
    "# - Single tool for TRAINING + INFERENCE\n",
    "# - Widely used in open-source models\n",
    "\n",
    "# -------------------------\n",
    "# 5. Multilingual handling\n",
    "# -------------------------\n",
    "# GPT-2 tokenizer:\n",
    "# - Language-agnostic via bytes\n",
    "# - Works equally for all scripts\n",
    "#\n",
    "# SentencePiece:\n",
    "# - Script-aware (can split by Unicode script)\n",
    "# - Better semantic segmentation for languages\n",
    "\n",
    "# -------------------------\n",
    "# 6. Token stability\n",
    "# -------------------------\n",
    "# GPT-2 tokenizer:\n",
    "# - Stable, deterministic, reversible\n",
    "# - Same bytes ‚Üí same tokens always\n",
    "#\n",
    "# SentencePiece:\n",
    "# - Depends on normalization + training config\n",
    "# - Small changes can alter token boundaries\n",
    "\n",
    "# -------------------------\n",
    "# 7. Vocabulary semantics\n",
    "# -------------------------\n",
    "# GPT-2 tokenizer:\n",
    "# - Tokens often represent byte patterns\n",
    "# - Less human-readable\n",
    "#\n",
    "# SentencePiece:\n",
    "# - Tokens represent characters/subwords\n",
    "# - More interpretable vocab\n",
    "\n",
    "# -------------------------\n",
    "# 8. Used by\n",
    "# -------------------------\n",
    "# GPT-2 tokenizer:\n",
    "# - GPT-2 / GPT-3 / GPT-4 (byte-level BPE)\n",
    "#\n",
    "# SentencePiece:\n",
    "# - LLaMA / LLaMA-2 / Mistral / T5\n",
    "\n",
    "# -------------------------\n",
    "# ONE-LINE SUMMARY\n",
    "# -------------------------\n",
    "# GPT-2 tokenizer = byte-level BPE (always safe, always reversible)\n",
    "# SentencePiece   = character-level BPE with optional byte fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# UTF-8 BYTES vs UNICODE CODE POINTS (CORE DIFF)\n",
    "# ===============================================\n",
    "\n",
    "# -------------------------\n",
    "# 1. What a Unicode code point is\n",
    "# -------------------------\n",
    "# A Unicode CODE POINT is an abstract number that identifies a character.\n",
    "#\n",
    "# Examples:\n",
    "#   'A'   -> U+0041\n",
    "#   '√©'   -> U+00E9\n",
    "#   '‰Ω†'  -> U+4F60\n",
    "#   'üòÑ'  -> U+1F604\n",
    "#\n",
    "# Code points are LANGUAGE-LEVEL concepts.\n",
    "# They do NOT define how characters are stored in memory.\n",
    "\n",
    "# -------------------------\n",
    "# 2. What UTF-8 bytes are\n",
    "# -------------------------\n",
    "# UTF-8 BYTES are the concrete binary encoding of a Unicode code point.\n",
    "#\n",
    "# UTF-8 represents each code point using 1‚Äì4 bytes.\n",
    "#\n",
    "# Examples:\n",
    "#   'A'   -> [0x41]\n",
    "#   '√©'   -> [0xC3, 0xA9]\n",
    "#   '‰Ω†'  -> [0xE4, 0xBD, 0xA0]\n",
    "#   'üòÑ'  -> [0xF0, 0x9F, 0x98, 0x84]\n",
    "#\n",
    "# Bytes are MACHINE-LEVEL storage units (0‚Äì255).\n",
    "\n",
    "# -------------------------\n",
    "# 3. Key difference (mental model)\n",
    "# -------------------------\n",
    "# Unicode code point:\n",
    "# - \"What character is this?\"\n",
    "#\n",
    "# UTF-8 byte:\n",
    "# - \"How is this character stored?\"\n",
    "\n",
    "# -------------------------\n",
    "# 4. One-to-many relationship\n",
    "# -------------------------\n",
    "# One Unicode code point -> MANY UTF-8 bytes\n",
    "#\n",
    "# This is why:\n",
    "# - len(\"üòÑ\")        == 1   (code points)\n",
    "# - len(\"üòÑ\".encode(\"utf-8\")) == 4   (bytes)\n",
    "\n",
    "# -------------------------\n",
    "# 5. Why this matters for tokenizers\n",
    "# -------------------------\n",
    "# Tokenizers must choose:\n",
    "#\n",
    "# A) Work on CODE POINTS\n",
    "#    - Cleaner linguistic units\n",
    "#    - Risk of <UNK> unless handled carefully\n",
    "#\n",
    "# B) Work on UTF-8 BYTES\n",
    "#    - Always reversible\n",
    "#    - No unknown characters\n",
    "#    - More low-level\n",
    "\n",
    "# -------------------------\n",
    "# 6. GPT vs LLaMA choice\n",
    "# -------------------------\n",
    "# GPT-style tokenizers:\n",
    "# - Tokenize UTF-8 BYTES\n",
    "#\n",
    "# LLaMA / SentencePiece:\n",
    "# - Tokenize Unicode CODE POINTS\n",
    "# - Fall back to UTF-8 bytes for rare chars\n",
    "\n",
    "# -------------------------\n",
    "# ONE-LINE SUMMARY\n",
    "# -------------------------\n",
    "# Unicode code points = meaning (characters)\n",
    "# UTF-8 bytes         = storage (binary encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15cc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UNSTABLE TOKENS ‚Äî WHAT THEY ARE AND WHY THEY BREAK LLMS\n",
    "# ============================================================\n",
    "\n",
    "# -------------------------\n",
    "# What are unstable tokens?\n",
    "# -------------------------\n",
    "# An unstable token is a token whose boundaries or meaning are\n",
    "# highly sensitive to:\n",
    "#   - surrounding whitespace\n",
    "#   - punctuation\n",
    "#   - capitalization\n",
    "#   - context (preceding/following characters)\n",
    "#\n",
    "# Small textual changes ‚Üí completely different token sequences.\n",
    "#\n",
    "# This instability propagates into model reasoning failures.\n",
    "\n",
    "# ============================================================\n",
    "# WHY THIS CAUSES REAL PROBLEMS (ONE BY ONE)\n",
    "# ============================================================\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Why can't LLM spell words?\n",
    "# ------------------------------------------------------------\n",
    "# LLMs do NOT see characters.\n",
    "# They see TOKENS (subwords / byte chunks).\n",
    "#\n",
    "# Example:\n",
    "#   \"banana\" might be tokenized as:\n",
    "#   [\"ba\", \"na\", \"na\"]\n",
    "#\n",
    "# The model never learns spelling character-by-character.\n",
    "# Asking it to spell = asking it to reason *across token boundaries*.\n",
    "#\n",
    "# => Tokenization destroys character-level continuity.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Why can't LLM reverse a string?\n",
    "# ------------------------------------------------------------\n",
    "# Reversing a string requires:\n",
    "#   - exact character order\n",
    "#\n",
    "# But tokens are variable-length chunks:\n",
    "#   \"hello\" -> [\"hel\", \"lo\"]\n",
    "#\n",
    "# Reverse tokens != reverse characters.\n",
    "#\n",
    "# The model cannot reliably reconstruct the original characters.\n",
    "#\n",
    "# => Tokenization is lossy with respect to string operations.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Why are LLMs worse at non-English languages (e.g., Japanese)?\n",
    "# ------------------------------------------------------------\n",
    "# Many non-English languages:\n",
    "#   - have no spaces\n",
    "#   - have large Unicode vocabularies\n",
    "#\n",
    "# Tokenizers:\n",
    "#   - fragment text poorly\n",
    "#   - produce longer token sequences\n",
    "#\n",
    "# Longer sequences ‚Üí more uncertainty ‚Üí worse performance.\n",
    "#\n",
    "# => Tokenization bias favors English.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Why is LLM bad at simple arithmetic?\n",
    "# ------------------------------------------------------------\n",
    "# Numbers are tokenized as text fragments:\n",
    "#   \"12345\" -> [\"12\", \"345\"] or worse\n",
    "#\n",
    "# Arithmetic requires digit-level reasoning.\n",
    "# Tokens destroy numeric structure.\n",
    "#\n",
    "# The model learns patterns, not math.\n",
    "#\n",
    "# => Tokenization breaks numerical compositionality.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Why did GPT-2 struggle with Python code?\n",
    "# ------------------------------------------------------------\n",
    "# Python syntax is whitespace-sensitive.\n",
    "#\n",
    "# GPT-2 tokenizer:\n",
    "#   - had unstable whitespace tokens\n",
    "#   - mixed spaces + tabs + newlines unpredictably\n",
    "#\n",
    "# Example:\n",
    "#   \"    \" vs \"\\t\" vs \" \\n\"\n",
    "#\n",
    "# Same meaning, different tokens.\n",
    "#\n",
    "# => Tokenization noise destroys syntactic precision.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Why does the LLM halt on \"<|endoftext|>\"?\n",
    "# ------------------------------------------------------------\n",
    "# \"<|endoftext|>\" is a SPECIAL TOKEN.\n",
    "#\n",
    "# If it appears in user input:\n",
    "#   - tokenizer maps it to a control token\n",
    "#   - model interprets it as \"STOP\"\n",
    "#\n",
    "# => Accidental control-token injection.\n",
    "#\n",
    "# This is NOT language understanding ‚Äî it's tokenizer control flow.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# What is the \"trailing whitespace\" warning?\n",
    "# ------------------------------------------------------------\n",
    "# Many tokenizers treat:\n",
    "#   \"word\"\n",
    "#   \"word \"\n",
    "#\n",
    "# as DIFFERENT TOKENS.\n",
    "#\n",
    "# Trailing whitespace may:\n",
    "#   - change token boundaries\n",
    "#   - alter probabilities\n",
    "#   - break structured outputs\n",
    "#\n",
    "# => Invisible characters ‚â† invisible effects.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Why does \"SolidGoldMagikarp\" break models?\n",
    "# ------------------------------------------------------------\n",
    "# Some strings appear RARELY but as SINGLE TOKENS in training.\n",
    "#\n",
    "# These tokens:\n",
    "#   - get weird, spiky embeddings\n",
    "#   - dominate model behavior\n",
    "#\n",
    "# The model \"overreacts\" to them.\n",
    "#\n",
    "# => Vocabulary artifacts leak into reasoning.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Why prefer YAML over JSON with LLMs?\n",
    "# ------------------------------------------------------------\n",
    "# JSON:\n",
    "#   - strict punctuation\n",
    "#   - commas, quotes, braces\n",
    "#\n",
    "# Tokenization errors = invalid JSON.\n",
    "#\n",
    "# YAML:\n",
    "#   - more forgiving\n",
    "#   - whitespace-tolerant\n",
    "#\n",
    "# => YAML is more robust to tokenization noise.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Why is LLM not end-to-end language modeling?\n",
    "# ------------------------------------------------------------\n",
    "# True end-to-end LM:\n",
    "#   characters -> model -> characters\n",
    "#\n",
    "# Reality:\n",
    "#   text -> tokenizer -> tokens -> model -> tokens -> detokenizer -> text\n",
    "#\n",
    "# Tokenizer is:\n",
    "#   - hand-engineered\n",
    "#   - frozen\n",
    "#   - non-learned\n",
    "#\n",
    "# => The model never sees raw language.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# What is the real root of suffering?\n",
    "# ------------------------------------------------------------\n",
    "# The tokenizer.\n",
    "#\n",
    "# It:\n",
    "#   - discretizes continuous language\n",
    "#   - introduces irreversible abstractions\n",
    "#   - creates edge cases everywhere\n",
    "#\n",
    "# LLMs are powerful DESPITE tokenization,\n",
    "# not because of it.\n",
    "\n",
    "# ============================================================\n",
    "# ONE-SENTENCE SUMMARY\n",
    "# ============================================================\n",
    "# Most LLM \"stupidity\" is not model failure ‚Äî it is tokenizer failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad84360",
   "metadata": {},
   "source": [
    "Interesting read : https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
